{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nab/anaconda3/envs/pyeed_niklas/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pyeed import Pyeed\n",
    "\n",
    "from pyeed.analysis.ontology_loading import OntologyAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¡ Connected to database.\n"
     ]
    }
   ],
   "source": [
    "uri = \"bolt://127.0.0.1:1123\"\n",
    "user = \"neo4j\"\n",
    "password = \"niklasonlytems\"\n",
    "\n",
    "# Create a Pyeed object, automatically connecting to the database\n",
    "eedb = Pyeed(uri, user, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 11:19:45,962 - INFO - Database stats: {'nodes': 9052, 'relationships': 16325}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the connection url is bolt://neo4j:niklasonlytems@127.0.0.1:1123\n",
      "Loaded /home/nab/Niklas/pyeed/src/pyeed/model.py\n",
      "Connecting to bolt://neo4j:niklasonlytems@127.0.0.1:1123\n",
      "Setting up indexes and constraints...\n",
      "\n",
      "Found model.StrictStructuredNode\n",
      " ! Skipping class model.StrictStructuredNode is abstract\n",
      "Found model.Organism\n",
      " + Creating node unique constraint for taxonomy_id on label Organism for class model.Organism\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent constraint already exists, 'Constraint( id=15, name='constraint_unique_Organism_taxonomy_id', type='UNIQUENESS', schema=(:Organism {taxonomy_id}), ownedIndex=14 )'.}\n",
      "Found model.Site\n",
      " + Creating node unique constraint for site_id on label Site for class model.Site\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent constraint already exists, 'Constraint( id=10, name='constraint_unique_Site_site_id', type='UNIQUENESS', schema=(:Site {site_id}), ownedIndex=9 )'.}\n",
      "Found model.Region\n",
      " + Creating node unique constraint for region_id on label Region for class model.Region\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent constraint already exists, 'Constraint( id=18, name='constraint_unique_Region_region_id', type='UNIQUENESS', schema=(:Region {region_id}), ownedIndex=17 )'.}\n",
      "Found model.StandardNumbering\n",
      "Found model.GOAnnotation\n",
      " + Creating node unique constraint for go_id on label GOAnnotation for class model.GOAnnotation\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent constraint already exists, 'Constraint( id=4, name='constraint_unique_GOAnnotation_go_id', type='UNIQUENESS', schema=(:GOAnnotation {go_id}), ownedIndex=3 )'.}\n",
      "Found model.Protein\n",
      " + Creating node unique constraint for accession_id on label Protein for class model.Protein\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent constraint already exists, 'Constraint( id=12, name='constraint_unique_Protein_accession_id', type='UNIQUENESS', schema=(:Protein {accession_id}), ownedIndex=11 )'.}\n",
      " + Creating vector index for embedding on label Protein for class model.Protein\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent index already exists, 'Index( id=7, name='vector_index_Protein_embedding', type='VECTOR', schema=(:Protein {embedding}), indexProvider='vector-2.0' )'.}\n",
      "Found model.DNA\n",
      " + Creating node unique constraint for accession_id on label DNA for class model.DNA\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent constraint already exists, 'Constraint( id=5, name='constraint_unique_DNA_accession_id', type='UNIQUENESS', schema=(:DNA {accession_id}), ownedIndex=8 )'.}\n",
      " + Creating vector index for embedding on label DNA for class model.DNA\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent index already exists, 'Index( id=6, name='vector_index_DNA_embedding', type='VECTOR', schema=(:DNA {embedding}), indexProvider='vector-2.0' )'.}\n",
      "Found model.OntologyObject\n",
      " + Creating node unique constraint for name on label OntologyObject for class model.OntologyObject\n",
      "{code: Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExists} {message: An equivalent constraint already exists, 'Constraint( id=13, name='constraint_unique_OntologyObject_name', type='UNIQUENESS', schema=(:OntologyObject {name}), ownedIndex=16 )'.}\n",
      "\n",
      "Finished 9 classes.\n",
      "âœ… Databse constraints and indexes set up according to Pyeed Graph Object Model.\n"
     ]
    }
   ],
   "source": [
    "# For testing purposes, we will wipe the database and remove all constraints\n",
    "# eedb.db.wipe_database(date='2024-12-13')\n",
    "# eedb.db.remove_db_constraints(user=user, password=password)\n",
    "\n",
    "# DB connector is an attribute of the Pyeed object, type `DatabaseConnector`\n",
    "LOGGER.info(f\"Database stats: {eedb.db.stats()}\")\n",
    "\n",
    "# The first time the pyeed database is initialized, we need to create the constraints which are defined in the pyeed graph model\n",
    "eedb.db.initialize_db_constraints(user=user, password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok we are ready to go\n",
    "LOGGER.info(\"Setup complete\")\n",
    "\n",
    "# read in the ids.json file form this directory\n",
    "with open(\"/home/nab/Niklas/TEM-lactamase/data/TEM_Ids/TEM_Ids.json\", \"r\") as f:\n",
    "    dict_id_name = json.load(f)\n",
    "\n",
    "# now fecth all of the proteins from the database\n",
    "eedb.fetch_from_primary_db(dict_id_name, db='ncbi_protein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 protein_name phenotype    protein_id protein_id_database\n",
      "0           0        TEM-1        2b      AAP20891          AAP20891.1\n",
      "1           1        TEM-2        2b      CAJ85677          CAJ85677.1\n",
      "2           2        TEM-3       2be      SAQ02853          SAQ02853.1\n",
      "3           3        TEM-4       2be      CDR98216          CDR98216.1\n",
      "4           4        TEM-5       2be  WP_109963600      WP_109963600.1\n"
     ]
    }
   ],
   "source": [
    "# read in the pandas dataframe\n",
    "df = pd.read_csv('/home/nab/Niklas/TEM-lactamase/data/002_combined_data/TEM_lactamase.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyeed.analysis.standard_numbering import StandardNumberingTool\n",
    "\n",
    "# Apply the standard numbering\n",
    "standard_numbering = StandardNumberingTool(name=\"test_standard_numbering_all\")\n",
    "standard_numbering.apply_standard_numbering(base_sequence_id='AAP20891.1', db=eedb.db) # , list_of_seq_ids=df['protein_id_database'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing protein 1 of 265\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/nab/anaconda3/envs/pyeed_niklas/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/nab/anaconda3/envs/pyeed_niklas/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The permutations of the neighbours including the base sequence are: 209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we want to start with a mutational detection\n",
    "# a first approach is to just include the 209 TEMs and see if we can detect the mutations\n",
    "# here we find the colsest neighbor based on the standard numbering and then we can find their mutations\n",
    "# we also want to coun the number of mutations, the idendeity, the cosine distance and the euclidean distance between all of them\n",
    "# we can therefore perform a pairwise alignment between the found neighbours\n",
    "\n",
    "# we first need to find the closest neighbour to the base sequence\n",
    "n_neighbours = 40000\n",
    "\n",
    "from pyeed.analysis.embedding_analysis import EmbeddingTool\n",
    "from pyeed.analysis.sequence_alignment import PairwiseAligner\n",
    "from pyeed.analysis.mutation_detection import MutationDetection\n",
    "\n",
    "et = EmbeddingTool()\n",
    "pa = PairwiseAligner()\n",
    "md = MutationDetection()\n",
    "\n",
    "# count the number of pairwise alignments performed\n",
    "# we want to expect 209*209 / 2 = 21801 pairwise alignments\n",
    "counter = 0\n",
    "already_processed_pairs = []\n",
    "\n",
    "# iterate over the different proteins ids in df\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Processing protein {index+1} of {len(df)}\")\n",
    "    # get the id in the database\n",
    "    base_sequence_id = row['protein_id_database']\n",
    "\n",
    "    closest_neighbours = et.find_closest_matches_simple(start_sequence_id=base_sequence_id, db=eedb.db, n = n_neighbours)\n",
    "    # print(f\"The number of closest neighbours is: {len(closest_neighbours)}\")\n",
    "\n",
    "    # the protein itself is returned as well\n",
    "    # the list is build up of tuples with the following structure: (sequence_id, distance)\n",
    "    closest_neighbours_ids = [neighbour[0] for neighbour in closest_neighbours]\n",
    "    # print(f\"The closest neighbours ids are: {closest_neighbours_ids}\")\n",
    "\n",
    "    # for the moment we only want to look at ids which are in the TEM-209 list\n",
    "    # this list is stored in the df dataframe\n",
    "    # we can get the ids from the df dataframe by using the 'protein_id_database' column\n",
    "    # we need to make sure that the ids are in the closest_neighbours_ids list\n",
    "    # we can do this by using the intersection of the two lists\n",
    "    tem_209_ids = df['protein_id_database'].tolist()\n",
    "    # print(f\"The TEM-209 ids are: {tem_209_ids}\")\n",
    "\n",
    "    # now we can get the intersection of the two lists\n",
    "    intersection = list(set(closest_neighbours_ids) & set(tem_209_ids))\n",
    "    # print(f\"The intersection of the two lists is: {len(intersection)}\")\n",
    "\n",
    "    # we need to create all of the permutations of the neighbours with the base sequence\n",
    "    # please that the reverse direction should not be included\n",
    "    # this means that the base sequence is always the first element in the tuple and the second element is the neighbour\n",
    "    permutations = [(base_sequence_id, neighbour) for neighbour in intersection]\n",
    "    print(f\"The permutations of the neighbours including the base sequence are: {len(permutations)}\")\n",
    "\n",
    "    # we now want to exclude the pairs that we already processed keeping in mind that we always add in the list both directions\n",
    "    permuations_to_process = [pair for pair in permutations if pair not in already_processed_pairs]\n",
    "\n",
    "    # we now update the already_processed_pairs list with the new pairs\n",
    "    # we need to add the reverse of the pair as well\n",
    "    already_processed_pairs.extend([(pair[1], pair[0]) for pair in permuations_to_process])\n",
    "    already_processed_pairs.extend(permuations_to_process)\n",
    "    \n",
    "    # now we run a pairwise alignment between the found neighbours\n",
    "    pairwise_alignment = pa.align_multipairwise(ids=intersection, db=eedb.db, pairs = permuations_to_process)\n",
    "    # print(f\"The pairwise alignment between the found neighbours and the base sequence is: {pairwise_alignment}\")\n",
    "    counter += len(permuations_to_process)\n",
    "\n",
    "    # now we detect the mutations\n",
    "    mutations = []\n",
    "    # now we can detect the mutations\n",
    "    for i in range(len(permuations_to_process)):\n",
    "        if permuations_to_process[i][0] == permuations_to_process[i][1]:\n",
    "            print(f\"Skipping permutation {i+1} of {len(permuations_to_process)} because they are the same\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"Mutation detection for permuations_to_process {i+1} of {len(permuations_to_process)} between {permuations_to_process[i][0]} and {permuations_to_process[i][1]}\")\n",
    "        result = md.get_mutations_between_sequences(sequence_id1=permuations_to_process[i][0], sequence_id2=permuations_to_process[i][1], db=eedb.db, save_to_db=True, standard_numbering_tool_name=\"test_standard_numbering\")\n",
    "        # print(f\"Number of mutations: {len(result)}\")\n",
    "\n",
    "        mutations.append(result)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"The number of pairwise alignments performed is: {counter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to analyze the mutations\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the sequences\n",
    "aligner = PairwiseAligner()\n",
    "\n",
    "# fetch all ids\n",
    "query = \"\"\"\n",
    "        MATCH (p:Protein) \n",
    "        WHERE p.accession_id IS NOT NULL\n",
    "        RETURN p.accession_id AS accession_id\n",
    "        \"\"\"\n",
    "ids = [record['accession_id'] for record in eedb.db.execute_read(query)]\n",
    "print(ids)\n",
    "\n",
    "aligner.align_multipairwise(db=eedb.db, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the DNA entries for the proteins\n",
    "eedb.fetch_dna_entries_for_proteins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to know how many of the TEM proteins have a DNA sequence linked to them\n",
    "# this can be found by checking if the DNA-[ENCODES]->Protein relationship exists\n",
    "# then it should be compared to the TEM-Proteins from the dict and their IDs checked so that we can see if all of them have a DNA sequence\n",
    "\n",
    "query = \"\"\"\n",
    "        MATCH (d:DNA)-[e:ENCODES]->(p:Protein)\n",
    "        WHERE p.accession_id IS NOT NULL\n",
    "        RETURN p.accession_id AS accession_id\n",
    "        \"\"\"\n",
    "dna_protein_ids = [record['accession_id'] for record in eedb.db.execute_read(query)]\n",
    "print(len(dna_protein_ids))\n",
    "# ['AQT03459.1', 'AFC75523.1', 'CAB92324.1', 'AAF01046.1', 'AFC75524.1']\n",
    "print(dna_protein_ids[:5])\n",
    "\n",
    "# first we need to get the ids from the dict\n",
    "dict_ids = list(dict_id_name.keys())\n",
    "print(len(dict_ids))\n",
    "# ['AAP20891', 'CAJ85677', 'SAQ02853', 'CDR98216', 'WP_109963600']\n",
    "print(dict_ids[:5])\n",
    "\n",
    "# but we need to be carful, because the dict_ids are not the same as the dna_protein_ids\n",
    "# the dict_ids are the ids without the version number, so we need to remove the version number from the dna_protein_ids\n",
    "dna_protein_ids = [id.split('.')[0] for id in dna_protein_ids]\n",
    "print(len(dna_protein_ids))\n",
    "print(dna_protein_ids[:5])\n",
    "\n",
    "# now we can compare the two lists\n",
    "diff_ids = list(set(dict_ids) - set(dna_protein_ids))\n",
    "print(len(diff_ids))\n",
    "print(diff_ids)\n",
    "\n",
    "# make the printout a bit more readable\n",
    "print(f\" {len(diff_ids)} of the TEM proteins do not have a DNA sequence linked to them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first need to compute the vector embedding for the proteins\n",
    "eedb.calculate_sequence_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the owl ontology\n",
    "file_path = \"/home/nab/Niklas/TEM-lactamase/CARD_Data_Ontologies/aro.owl\"\n",
    "db = eedb.db\n",
    "ontology_adapter = OntologyAdapter()\n",
    "\n",
    "ontology_adapter.import_ontology_file_in_db(file_path, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to pull all of the proteins that are in the CARD ontology, and link them to the ontology structure\n",
    "# we now open the tsv index file from CARD and link the proteins to the ontology, but first we have to pull them\n",
    "# ARO Accession\tCVTERM ID\tModel Sequence ID\tModel ID\tModel Name\tARO Name\tProtein Accession\tDNA Accession\tAMR Gene Family\tDrug Class\tResistance Mechanism\tCARD Short Name\n",
    "file_path = \"/home/nab/Niklas/TEM-lactamase/CARD_Data_Data/aro_index.tsv\"\n",
    "\n",
    "# open the file and read in the proteins\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "df = df.dropna(subset=[\"Protein Accession\"])\n",
    "\n",
    "# now we want to fetch the proteins from the database\n",
    "# eedb.fetch_from_primary_db(df[\"Protein Accession\"].tolist(), db='ncbi_protein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to link the proteins to the ontology\n",
    "# we do this by matching the protein accession and the ARO Accession\n",
    "# the link realtionship is the following:     go_annotation = RelationshipTo(\"GOAnnotation\", \"ASSOCIATED_WITH\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # the query is the following to match and to link the protein to the ontology\n",
    "    # it is in cypther since we are using the neo4j database\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Protein {accession_id: $protein_accession})\n",
    "    MATCH (a:OntologyObject {name: $aro_accession})\n",
    "    MERGE (p)-[:ASSOCIATED_WITH]->(a)\n",
    "    \"\"\"\n",
    "\n",
    "    # we now execute the query\n",
    "    # example ARO:3002527 need to be name: http://purl.obolibrary.org/obo/ARO_3002527\n",
    "    eedb.db.execute_write(query, parameters={\"protein_accession\": row[\"Protein Accession\"], \"aro_accession\": f\"http://purl.obolibrary.org/obo/{row['ARO Accession'].replace(':', '_')}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyeed_niklas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
